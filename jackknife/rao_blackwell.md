# 条件付き平均

「条件付き平均（conditional expectation）」とは、
「ある条件のもとで平均をとること」を意味します。
統計や確率論ではとても基本的な概念です。

🧩 定義（確率的にいうと）

確率変数 𝑋 と 𝑌 があるとき、「𝑌 の値が分かっているときの 𝑋 の平均値」を
条件付き期待値と呼び、記号で次のように書きます：

𝐸[𝑋∣𝑌]

つまり：
「いま 𝑌 の値を観測した。その情報を考慮して、𝑋 の平均値をどう見積もるか？」
を表す量です。

🎲 直感的な例

コインを投げて2つの変数を考えます。

X：投げた回数のうち表の数（0〜n）
Y：少し粗い情報（たとえば「偶数回目に表が出たか」など）

すると、もし「𝑌=1」という情報がわかっているなら、

𝑋 の取りうる範囲が狭まります。
その限定された範囲の中での平均を取る ——
それが「条件付き平均（条件付き期待値）」です。

# Rao–Blackwell化

Rao–Blackwell化は、ざっくり言うと

「すでに持っている推定量を、情報のまとめ方を工夫することで “もっと良くする” 一般レシピ」

です。ジャックナイフ透過率推定は、

まず「いい感じだけどバイアスのある推定量」を作って

それを Rao–Blackwell化して UMVU（最小分散無バイアス推定量）にする

という流れになっています。

ここでは：

Rao–Blackwellの定理そのもの

直感的イメージ

簡単な具体例（コイン投げ）

「完全十分統計量」とUMVUとの関係

この論文（Jackknife Transmittance）での使われ方

の順で、論文外の情報も含めて説明します。

1. Rao–Blackwellの定理とは？

統計学の古典的な定理で、ざっくり言えば：

あるパラメータ 
𝜃
θ の推定量 
𝑇
(
𝑋
)
T(X) があったとき、

𝑇
(
𝑋
)
T(X) を 十分統計量 
𝑆
(
𝑋
)
S(X) による条件付き期待値に置き換えると

無バイアス性は保たれ

分散は必ず小さくなる（＝精度が良くなる）

というものです。教科書だと次のように書かれます （計算はダミー、内容は一般知識から）：

𝑋
X：観測データ

𝜃
θ：知りたいパラメータ

𝑇
(
𝑋
)
T(X)：
𝜃
θ の無バイアス推定量（
𝐸
[
𝑇
(
𝑋
)
]
=
𝜃
E[T(X)]=θ）

𝑆
(
𝑋
)
S(X)：
𝜃
θ に関する「十分統計量（sufficient statistic）」

すると
Rao–Blackwell推定量 
𝑇
∗
(
𝑋
)
:
=
𝐸
[
𝑇
(
𝑋
)
∣
𝑆
(
𝑋
)
]
T
∗
(X):=E[T(X)∣S(X)] は

𝐸
[
𝑇
∗
(
𝑋
)
]
=
𝜃
E[T
∗
(X)]=θ（やっぱり無バイアス）

V
a
r
(
𝑇
∗
(
𝑋
)
)
≤
V
a
r
(
𝑇
(
𝑋
)
)
Var(T
∗
(X))≤Var(T(X))（分散は小さくなる）

という性質を持ちます（標準的な統計テキストやWikipedia等に同内容があります）

さらに Lehmann–Schefféの定理を合わせると：

𝑆
(
𝑋
)
S(X) が「完全な十分統計量（complete sufficient statistic）」なら、
こうして得た 
𝐸
[
𝑇
(
𝑋
)
∣
𝑆
(
𝑋
)
]
E[T(X)∣S(X)] は 唯一の UMVU 推定量 になる。

というところまで行きます。

2. 直感的イメージ

もう少し感覚的にいうと：

生の観測データ 
𝑋
=
(
𝑋
0
,
…
,
𝑋
𝑚
−
1
)
X=(X
0
	​

,…,X
m−1
	​

) には、
同じ情報が「重複」して入っていることが多い。

統計的に見ると、「そのパラメータに関して必要な情報はこの値だけ見れば十分」
という凝縮形があって、それが 十分統計量 
𝑆
(
𝑋
)
S(X)。

Rao–Blackwell化は

「とりあえず作った推定量を、十分統計量で条件付き平均を取ることにより、
ムダなランダムネスを削って、情報だけ抽出する」

操作だと思うとイメージしやすいです。

条件付き期待値を取る ⇒ ランダムネスを平均化して“ブレ”を減らす

でも「十分統計量で条件付け」しているので、
パラメータに関する情報は失われない（だから無バイアスのまま）

3. コイン投げの簡単な例（直感を付ける）
問題設定

コインを 
𝑛
n 回投げる

表の確率を 
𝜃
θ とする（推定したい）

観測は 
𝑋
1
,
…
,
𝑋
𝑛
∈
{
0
,
1
}
X
1
	​

,…,X
n
	​

∈{0,1}（1が表）

自然な推定量は標本平均：

𝜃
^
mean
=
1
𝑛
∑
𝑖
=
1
𝑛
𝑋
𝑖
θ
^
mean
	​

=
n
1
	​

i=1
∑
n
	​

X
i
	​


これはすでに UMVU になっているのですが、
Rao–Blackwellのイメージのために「わざと変な推定量」を使います。

変な推定量を作る

例えば：

「最初の1つだけ見る」推定量

𝑇
(
𝑋
)
=
𝑋
1
T(X)=X
1
	​


これは

𝐸
[
𝑇
(
𝑋
)
]
=
𝜃
E[T(X)]=θ なので無バイアスだが

分散は 
𝜃
(
1
−
𝜃
)
θ(1−θ) とわりと大きい

十分統計量で条件付き平均を取る

このとき、
𝜃
θ に関する十分統計量は「表の総数」：

𝑆
(
𝑋
)
=
∑
𝑖
=
1
𝑛
𝑋
𝑖
S(X)=
i=1
∑
n
	​

X
i
	​


Rao–Blackwell化では

𝑇
∗
(
𝑋
)
=
𝐸
[
𝑇
(
𝑋
)
∣
𝑆
(
𝑋
)
]
T
∗
(X)=E[T(X)∣S(X)]

を考えます。

ここで：

𝑆
=
𝑠
S=s が与えられている

その条件のもとで、 
𝑋
1
X
1
	​

 が 1 である確率は対称性から 
𝑠
/
𝑛
s/n

したがって

𝑇
∗
(
𝑋
)
=
𝐸
[
𝑋
1
∣
𝑆
]
=
𝑆
𝑛
T
∗
(X)=E[X
1
	​

∣S]=
n
S
	​


これはちょうど標本平均 
𝜃
^
mean
θ
^
mean
	​

 になっていて、
実際に 分散も 
𝑇
(
𝑋
)
T(X) より小さいです。

これが Rao–Blackwell 化の典型例で、

「適当な無バイアス推定量 
𝑇
T からスタート → 十分統計量で条件付き期待値をとる → 標本平均という“最良の推定量”が自然に出てくる」

という流れになっています。

4. 完全十分統計量と UMVU

統計の理論では：

𝑆
(
𝑋
)
S(X) が「完全（complete）」かつ「十分（sufficient）」な統計量で、

𝑇
(
𝑋
)
T(X) が任意の無バイアス推定量なら、

𝑇
∗
(
𝑋
)
=
𝐸
[
𝑇
(
𝑋
)
∣
𝑆
(
𝑋
)
]
T
∗
(X)=E[T(X)∣S(X)]

は「そのパラメータの UMVU 推定量として一意」になります。

正規分布の族（平均と分散が未知）の場合：

サンプル平均 
𝑋
ˉ
X
ˉ
 とサンプル分散 
𝑆
2
S
2
 の組

(
𝑋
ˉ
,
𝑆
2
)
(
X
ˉ
,S
2
)
が、平均や分散に対して完全十分統計量になることが知られています

これが Jackknife Transmittance 論文で効いてくるポイントです。

